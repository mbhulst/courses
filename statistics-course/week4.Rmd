---
title: "Summary Statistics & R"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Week 4

## Scatterplots

### Lecture

Scatterplots are used to study two dimensional data, such as the father/son heigth dataset. 

```{r, include=FALSE}
library(UsingR)
```

```{r}
data("father.son")
x=father.son$fheight
y=father.son$sheight
plot(x,y,xlab="Father's height in inches",ylab="Son's height in inches",main=paste("correlation =",signif(cor(x,y),2)))
```

Stratification followed by boxplots lets us see the distribution of each group. The average height of sons with fathers that are 72 is 70.7. We also see that the means of the strata appear to follow a straight line. This line is refereed to the regression line and itâ€™s slope is related to the correlation.

```{r}
groups <- split(y,round(x)) 
boxplot(groups)
```

### Exercizes

Here we will use the plots we've learned about to explore a dataset: some stats on a random sample of runners from the New York City Marthon in 2002. Use **dplyr** to create two new data frames: `males` and `females`, with the data for each gender. For males, what is the Pearson correlation between age and time to finish? For females, what is the Pearson correlation between age and time to finish?

If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender. Look at scatterplots and boxplots of times stratified by age groups (20-25, 25-30, etc..).

```{r}
data(nym.2002, package="UsingR")
library(dplyr)
males <- filter(nym.2002, gender=="Male")
females <- filter(nym.2002, gender=="Female")
x <- males$age
y <- males$time
library(rafalib)
mypar(2,2)
plot(x,y,xlab="Male's age",ylab="Male's time",main=paste("correlation =",signif(cor(x,y),3)))
plot(x,y,xlab="Female's age",ylab="Female's time",main=paste("correlation =",signif(cor(x,y),3)))
group <- floor(females$age/5) * 5
boxplot(females$time~group)
group <- floor(males$age/5) * 5
boxplot(males$time~group)
```

Finish times are constant up through around 50-60, then they get slower.


## Symmetry of Log Ratios

### Exercizes

In the previous video, we saw that multiplicative changes are symmetric around 0 when we are on the logarithmic scale. In other words, if we use the log scale, 1/2 times a number x, and 2 times a number x, are equally far away from x. We will explore this with the NYC marathon data used in the previous assessment. What is the fastest time divided by the median time? What is the slowest time divided by the median time?

```{r}
data(nym.2002, package="UsingR")
time = sort(nym.2002$time)
max(time)/median(time)
min(time)/median(time)
```

Compare the following two plots. Note that the lines are equally spaced in Figure #2.

1) A plot of the ratio of times to the median time, with horizontal lines at twice as fast as the median time, and twice as slow as the median time.
2) A plot of the log2 ratio of times to the median time. The horizontal lines indicate the same as above: twice as fast and twice as slow.
```{r}
library(rafalib)
mypar(1,2)
plot(time/median(time), ylim=c(1/4,4))
abline(h=c(1/2,1,2))
plot(log2(time/median(time)),ylim=c(-2,2))
abline(h=-1:1)
```


## Median, MAD and Spearman Correlation

### Exercizes

We are going to explore the properties of robust statistics. We will use one of the datasets included in R, which contains weight of chicks in grams as they grow from day 0 to day 21. This dataset also splits up the chicks by different protein diets, which are coded from 1 to 4. We use this dataset to also show an important operation in R (not related to robust summaries): `reshape.`



```{r}
data(ChickWeight)
head(ChickWeight)
plot( ChickWeight$Time, ChickWeight$weight, col=ChickWeight$Diet)
```

Notice that the rows here represent time points rather than individuals. To facilitate the comparison of weights at different time points and across the different chicks, we will reshape the data so that each row is a chick. In R we can do this with the reshape function:

```{r}
chick = reshape(ChickWeight, idvar=c("Chick","Diet"), timevar="Time",
                direction="wide")
head(chick)
chick = na.omit(chick)
```

Focus on the chick weights on day 4 (check the column names of `chick` and note the numbers). How much does the average of chick weights at day 4 increase if we add an outlier measurement of 3000 grams? Specifically, what is the average weight of the day 4 chicks, including the outlier chick, divided by the average of the weight of the day 4 chicks without the outlier. Hint: use `c()` to add a number to a vector.

```{r}
mean(c(chick$weight.4, 3000))/mean(chick$weight.4)
```

In exercise 1, we saw how sensitive the mean is to outliers. Now let's see what happens when we use the median instead of the mean. Compute the same ratio, but now using median instead of mean. Specifically, what is the median weight of the day 4 chicks, including the outlier chick, divided by the median of the weight of the day 4 chicks without the outlier.


```{r}
median(c(chick$weight.4, 3000))/median(chick$weight.4)
```

Now try the same thing with the sample standard deviation (the `sd()` function in R). Add a chick with weight 3000 grams to the chick weights from day 4. How much does the standard deviation change? What's the standard deviation with the outlier chick divided by the standard deviation without the outlier chick?

```{r}
sd(c(chick$weight.4, 3000))/sd(chick$weight.4)
```

Compare the result above to the median absolute deviation in R, which is calculated with the `mad()` function. Note that the MAD is unaffected by the addition of a single outlier. The `mad()` function in R includes the scaling factor 1.4826, such that `mad()` and `sd()` are very similar for a sample from a normal distribution.

What's the MAD with the outlier chick divided by the MAD without the outlier chick?

```{r}
mad(c(chick$weight.4, 3000))/mad(chick$weight.4)
```

Our last question relates to how the Pearson correlation is affected by an outlier as compared to the Spearman correlation. The Pearson correlation between x and y is given in R by `cor(x,y)`. The Spearman correlation is given by `cor(x,y,method="spearman")`.

Plot the weights of chicks from day 4 and day 21. We can see that there is some general trend, with the lower weight chicks on day 4 having low weight again on day 21, and likewise for the high weight chicks.

Calculate the Pearson correlation of the weights of chicks from day 4 and day 21. Now calculate how much the Pearson correlation changes if we add a chick that weighs 3000 on day 4 and 3000 on day 21. Again, divide the Pearson correlation with the outlier chick over the Pearson correlation computed without the outliers.

```{r}
plot(chick$weight.4, chick$weight.21)
plot(c(chick$weight.4,3000), c(chick$weight.21,3000))
cor(c(chick$weight.4,3000), c(chick$weight.21,3000))/cor(chick$weight.4, chick$weight.21)
```

## Mann-Whitney-Wilcoxon Text

### Exercizes

We are using the same dataset of chicks as described above.

```{r}
data(ChickWeight)
chick = reshape(ChickWeight, idvar=c("Chick","Diet"), timevar="Time", direction="wide")
chick = na.omit(chick)
```

Save the weights of the chicks on day 4 from diet 1 as a vector `x`. Save the weights of the chicks on day 4 from diet 4 as a vector `y`. Perform a t-test comparing `x` and `y` (in R the function `t.test(x,y)` will perform the test). Then perform a Wilcoxon test of `x` and `y` (in R the function `wilcox.test(x,y)` will perform the test). A warning will appear that an exact p-value cannot be calculated with ties, so an approximation is used, which is fine for our purposes.

```{r}
library(dplyr)
x <- filter(chick, Diet == 1) %>% select(weight.4) %>% unlist
y <- filter(chick, Diet == 4) %>% select(weight.4) %>% unlist
t.test(x,y)
wilcox.test(x,y)
```

Perform a t-test of x and y, after adding a single chick of weight 200 grams to `x` (the diet 1 chicks). What is the p-value from this test? The p-value of a test is available with the following code: `t.test(x,y)$p.value`

Do the same for the Wilcoxon test. The Wilcoxon test is robust to the outlier. In addition, it has less assumptions that the t-test on the distribution of the underlying data.

```{r}
t.test(c(x,200),y)$p.value
wilcox.test(c(x,200),y)$p.value
```

We will now investigate a possible downside to the Wilcoxon-Mann-Whitney test statistic. Using the following code to make three boxplots, showing the true Diet 1 vs 4 weights, and then two altered versions: one with an additional difference of 10 grams and one with an additional difference of 100 grams. Use the x and y as defined above, NOT the ones with the added outlier.

```{r}
library(rafalib)
mypar(1,3)
boxplot(x,y)
boxplot(x,y+10)
boxplot(x,y+100)
```

What is the difference in t-test statistic (obtained by `t.test(x,y)$statistic`) between adding 10 and adding 100 to all the values in the group `y`? Take the the t-test statistic with x and y+10 and subtract the t-test statistic with x and y+100. The value should be positive.

```{r}
t.test(x,y+10)$statistic - t.test(x,y+100)$statistic
```

Examine the Wilcoxon test statistic for x and y+10 and for x and y+100. Because the Wilcoxon works on ranks, once the two groups show complete separation, that is all points from group y are above all points from group x, the statistic will not change, regardless of how large the difference grows. Likewise, the p-value has a minimum value, regardless of how far apart the groups are. This means that the Wilcoxon test can be considered less powerful than the t-test in certain contexts. In fact, for small sample sizes, the p-value can't be very small, even when the difference is very large.

What is the p-value if we compare c(1,2,3) to c(4,5,6) using a Wilcoxon test?

```{r}
wilcox.test(c(1,2,3),c(4,5,6))$p.value
```

What is the p-value if we compare c(1,2,3) to c(400,500,600) using a Wilcoxon test?


```{r}
wilcox.test(c(1,2,3),c(400,500,600))$p.value
```



